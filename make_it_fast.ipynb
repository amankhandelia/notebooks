{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.replicate import get_audio_segments\n",
    "from transformers import Wav2Vec2ForAudioFrameClassification\n",
    "from transformers.models.wav2vec2.modeling_flax_wav2vec2 import FlaxWav2Vec2ForAudioFrameClassification\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import torch\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"/dev/shm/namo_speeches/wdtp42dMQkc.wav\"\n",
    "_, segments, _ = get_audio_segments(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_torch = [torch.tensor(np.asarray(segment)) for segment in segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "varied_segments = [segments[0], segments[1], segments[-1]]\n",
    "varied_segments_torch = [segments_torch[0], segments_torch[1], segments_torch[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-08-02 00:03:55.121\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mSanity check: [(2, 528000), (2, 576000), (2, 443736)]\u001b[0m\n",
      "\u001b[32m2023-08-02 00:03:55.123\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mSanity check (torch): [torch.Size([2, 528000]), torch.Size([2, 576000]), torch.Size([2, 443736])]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Sanity check: {[segment.shape for segment in varied_segments]}\")\n",
    "logger.info(f\"Sanity check (torch): {[segment.shape for segment in varied_segments_torch]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlaxWav2Vec2ForAudioFrameClassification.from_pretrained(\"/home/khandelia1000/speech_alignment/mms_alignment_model\", from_pt=True)\n",
    "torch_model = Wav2Vec2ForAudioFrameClassification.from_pretrained(\"/home/khandelia1000/speech_alignment/mms_alignment_model\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_output = [model(segment).logits for segment in varied_segments]\n",
    "with torch.inference_mode():\n",
    "    expected_output_torch = [torch_model(segment).logits for segment in varied_segments_torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1649, 31)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_output[0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test can we only send single channel input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = model(jnp.expand_dims(varied_segments[0][0], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlaxMaskedLMOutput(logits=Array([[[  9.573059 , -23.436459 , -23.549742 , ...,  -3.077409 ,\n",
       "          -4.258169 ,  -3.5700355],\n",
       "        [  9.764209 , -24.141933 , -24.319227 , ...,  -3.166452 ,\n",
       "          -4.5177855,  -3.5430603],\n",
       "        [  9.780839 , -24.496546 , -24.746893 , ...,  -3.5633605,\n",
       "          -4.8595405,  -3.7069871],\n",
       "        ...,\n",
       "        [  3.6285856, -19.138418 , -19.040558 , ...,  -2.500242 ,\n",
       "          -3.600256 ,  -4.0279903],\n",
       "        [  3.674955 , -19.257603 , -19.213037 , ...,  -2.3330367,\n",
       "          -3.8337636,  -4.350674 ],\n",
       "        [  3.7082536, -19.360212 , -19.371828 , ...,  -2.2507215,\n",
       "          -3.873837 ,  -4.3663855]]], dtype=float32), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[  9.569984 , -23.43094  , -23.54404  , ...,  -3.0728865,\n",
       "         -4.1720247,  -3.5677764],\n",
       "       [  9.778904 , -24.183798 , -24.358864 , ...,  -3.1577458,\n",
       "         -4.4343953,  -3.5652158],\n",
       "       [  9.80444  , -24.534115 , -24.776663 , ...,  -3.5720603,\n",
       "         -4.7612867,  -3.6760964],\n",
       "       ...,\n",
       "       [  3.6246803, -19.143164 , -19.045126 , ...,  -2.4978912,\n",
       "         -3.60352  ,  -4.043333 ],\n",
       "       [  3.6692128, -19.255657 , -19.211208 , ...,  -2.3258145,\n",
       "         -3.8385103,  -4.3620725],\n",
       "       [  3.7103   , -19.379566 , -19.391365 , ...,  -2.2478476,\n",
       "         -3.8891234,  -4.3864927]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_output[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1.105238, dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.max(expected_output[0][0] - test_output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(False, dtype=bool)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.allclose(expected_output[0][0], test_output.logits, atol=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForAudioFrameClassification(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=31, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.inference_mode:\n",
    "    torch_model.wav2vec2.feature_extractor(torch.tensor(np.asarray()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use mask for batched inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first without attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_input = torch.zeros(2, 576000)\n",
    "padded_input[:, :varied_segments_torch[0].shape[-1]] = varied_segments_torch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    padded_output = torch_model(padded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1799, 31]), torch.Size([2, 1649, 31]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_output.logits.shape, expected_output_torch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_count = expected_output_torch[0].shape[1]\n",
    "torch.allclose(expected_output_torch[0][:, :750, :], padded_output.logits[:, :750, :], atol=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5335)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(expected_output_torch[0][:, :1000, :] - padded_output.logits[:, :1000, :])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now with attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_input = torch.zeros(2, 576000)\n",
    "padded_input[:, :varied_segments_torch[0].shape[-1]] = varied_segments_torch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = torch.zeros(2, 576000)\n",
    "attention_mask[:, :varied_segments_torch[0].shape[-1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    padded_output = torch_model(padded_input, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_count = expected_output_torch[0].shape[1]\n",
    "torch.allclose(expected_output_torch[0][:, :frame_count, :], padded_output.logits[:, :frame_count, :], atol=1e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now padding with attention mask using flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_input = np.zeros((2, 576000))\n",
    "padded_input[:, :varied_segments_torch[0].shape[-1]] = varied_segments_torch[0]\n",
    "padded_input = jnp.asarray(padded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = np.zeros((2, 576000))\n",
    "attention_mask[:, :varied_segments_torch[0].shape[-1]] = 1\n",
    "attention_mask = jnp.asarray(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_output = model(padded_input, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[  9.569984 , -23.43094  , -23.54404  , ...,  -3.0728865,\n",
       "          -4.1720247,  -3.5677764],\n",
       "        [  9.778904 , -24.183798 , -24.358864 , ...,  -3.1577458,\n",
       "          -4.4343953,  -3.5652158],\n",
       "        [  9.80444  , -24.534115 , -24.776663 , ...,  -3.5720603,\n",
       "          -4.7612867,  -3.6760964],\n",
       "        ...,\n",
       "        [  3.474959 , -19.955288 , -19.794453 , ...,  -2.830078 ,\n",
       "          -3.4667163,  -3.1838577],\n",
       "        [  3.474959 , -19.955288 , -19.794453 , ...,  -2.830078 ,\n",
       "          -3.4667163,  -3.1838577],\n",
       "        [  3.474959 , -19.955288 , -19.794453 , ...,  -2.830078 ,\n",
       "          -3.4667163,  -3.1838577]],\n",
       "\n",
       "       [[  9.609749 , -23.410473 , -23.534405 , ...,  -2.9833636,\n",
       "          -4.216002 ,  -3.5279503],\n",
       "        [  9.823359 , -24.165634 , -24.348644 , ...,  -3.042981 ,\n",
       "          -4.466457 ,  -3.4635952],\n",
       "        [  9.805192 , -24.40877  , -24.656698 , ...,  -3.4314072,\n",
       "          -4.7778716,  -3.59754  ],\n",
       "        ...,\n",
       "        [  3.4676347, -19.934145 , -19.768532 , ...,  -2.8854613,\n",
       "          -3.4471188,  -3.1499605],\n",
       "        [  3.4676347, -19.934145 , -19.768532 , ...,  -2.8854613,\n",
       "          -3.4471188,  -3.1499605],\n",
       "        [  3.4676347, -19.934145 , -19.768532 , ...,  -2.8854613,\n",
       "          -3.4471188,  -3.1499605]]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_count = expected_output[0].shape[1]\n",
    "jnp.allclose(expected_output[0][:, :frame_count, :], padded_output.logits[:, :frame_count, :], atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[  3.6172302, -19.417892 , -19.459484 , ...,  -2.2876465,\n",
       "          -3.8989465,  -4.268706 ],\n",
       "        [  3.6149106, -19.461922 , -19.512568 , ...,  -2.3551292,\n",
       "          -3.930071 ,  -4.3022866],\n",
       "        [  3.6141102, -19.50552  , -19.576159 , ...,  -2.3266792,\n",
       "          -3.9247055,  -4.282878 ],\n",
       "        ...,\n",
       "        [  3.474959 , -19.955288 , -19.794453 , ...,  -2.830078 ,\n",
       "          -3.4667163,  -3.1838577],\n",
       "        [  3.474959 , -19.955288 , -19.794453 , ...,  -2.830078 ,\n",
       "          -3.4667163,  -3.1838577],\n",
       "        [  3.474959 , -19.955288 , -19.794453 , ...,  -2.830078 ,\n",
       "          -3.4667163,  -3.1838577]],\n",
       "\n",
       "       [[  3.5924635, -19.371904 , -19.414928 , ...,  -2.2873223,\n",
       "          -3.8779235,  -4.2652316],\n",
       "        [  3.5869386, -19.411625 , -19.463875 , ...,  -2.3564184,\n",
       "          -3.9051268,  -4.294119 ],\n",
       "        [  3.587076 , -19.47328  , -19.543835 , ...,  -2.3340583,\n",
       "          -3.9072373,  -4.2731466],\n",
       "        ...,\n",
       "        [  3.4676347, -19.934145 , -19.768532 , ...,  -2.8854613,\n",
       "          -3.4471188,  -3.1499605],\n",
       "        [  3.4676347, -19.934145 , -19.768532 , ...,  -2.8854613,\n",
       "          -3.4471188,  -3.1499605],\n",
       "        [  3.4676347, -19.934145 , -19.768532 , ...,  -2.8854613,\n",
       "          -3.4471188,  -3.1499605]]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_output.logits[:, frame_count:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
